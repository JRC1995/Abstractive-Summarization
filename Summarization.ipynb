{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [Seq2seq learning](https://arxiv.org/abs/1409.3215)\n",
    "with [attention mechanism](https://arxiv.org/abs/1409.0473), specifically [local attention](https://nlp.stanford.edu/pubs/emnlp15_attn.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pre-processed Dataset\n",
    "\n",
    "The Data is preprocessed in [Data_Pre-Processing.ipynb](https://github.com/JRC1995/Abstractive-Summarization/blob/master/Data_Pre-Processing.ipynb)\n",
    "\n",
    "Dataset source: https://www.kaggle.com/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('Processed_Data/Amazon_Reviews_Processed.json') as file:\n",
    "\n",
    "    for json_data in file:\n",
    "        saved_data = json.loads(json_data)\n",
    "\n",
    "        vocab2idx = saved_data[\"vocab\"]\n",
    "        embd = saved_data[\"embd\"]\n",
    "        train_batches_text = saved_data[\"train_batches_text\"]\n",
    "        test_batches_text = saved_data[\"test_batches_text\"]\n",
    "        val_batches_text = saved_data[\"val_batches_text\"]\n",
    "        train_batches_summary = saved_data[\"train_batches_summary\"]\n",
    "        test_batches_summary = saved_data[\"test_batches_summary\"]\n",
    "        val_batches_summary = saved_data[\"val_batches_summary\"]\n",
    "        train_batches_true_text_len = saved_data[\"train_batches_true_text_len\"]\n",
    "        val_batches_true_text_len = saved_data[\"val_batches_true_text_len\"]\n",
    "        test_batches_true_text_len = saved_data[\"test_batches_true_text_len\"]\n",
    "        train_batches_true_summary_len = saved_data[\"train_batches_true_summary_len\"]\n",
    "        val_batches_true_summary_len = saved_data[\"val_batches_true_summary_len\"]\n",
    "        test_batches_true_summary_len = saved_data[\"test_batches_true_summary_len\"]\n",
    "\n",
    "        break\n",
    "        \n",
    "idx2vocab = {v:k for k,v in vocab2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "max_summary_len = 31 # should be summary_max_len as used in data_preprocessing with +1 (+1 for <EOS>) \n",
    "D = 5 # D determines local attention window size\n",
    "window_len = 2*D+1\n",
    "l2=1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jishnu/miniconda3/envs/ML/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf \n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "embd_dim = len(embd[0])\n",
    "\n",
    "tf_text = tf.placeholder(tf.int32, [None, None])\n",
    "tf_embd = tf.placeholder(tf.float32, [len(vocab2idx),embd_dim])\n",
    "tf_true_summary_len = tf.placeholder(tf.int32, [None])\n",
    "tf_summary = tf.placeholder(tf.int32,[None, None])\n",
    "tf_train = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x,rate,training):\n",
    "    return tf.cond(tf_train,\n",
    "                    lambda: tf.nn.dropout(x,rate=0.3),\n",
    "                    lambda: x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed vectorized text\n",
    "\n",
    "Dropout used for regularization \n",
    "(https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_text = tf.nn.embedding_lookup(tf_embd, tf_text)\n",
    "\n",
    "embd_text = dropout(embd_text,rate=0.3,training=tf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM function\n",
    "\n",
    "More info: \n",
    "<br>\n",
    "https://dl.acm.org/citation.cfm?id=1246450, \n",
    "<br>\n",
    "https://www.bioinf.jku.at/publications/older/2604.pdf,\n",
    "<br>\n",
    "https://en.wikipedia.org/wiki/Long_short-term_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(x,hidden_state,cell,input_dim,hidden_size,scope):\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        w = tf.get_variable(\"w\", shape=[4,input_dim,hidden_size],\n",
    "                                    dtype=tf.float32,\n",
    "                                    trainable=True,\n",
    "                                    initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "        u = tf.get_variable(\"u\", shape=[4,hidden_size,hidden_size],\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=True,\n",
    "                            initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "        b = tf.get_variable(\"bias\", shape=[4,1,hidden_size],\n",
    "                    dtype=tf.float32,\n",
    "                    trainable=True,\n",
    "                    initializer=tf.zeros_initializer())\n",
    "        \n",
    "    input_gate = tf.nn.sigmoid( tf.matmul(x,w[0]) + tf.matmul(hidden_state,u[0]) + b[0])\n",
    "    forget_gate = tf.nn.sigmoid( tf.matmul(x,w[1]) + tf.matmul(hidden_state,u[1]) + b[1])\n",
    "    output_gate = tf.nn.sigmoid( tf.matmul(x,w[2]) + tf.matmul(hidden_state,u[2]) + b[2])\n",
    "    cell_ = tf.nn.tanh( tf.matmul(x,w[3]) + tf.matmul(hidden_state,u[3]) + b[3])\n",
    "    cell = forget_gate*cell + input_gate*cell_\n",
    "    hidden_state = output_gate*tf.tanh(cell)\n",
    "    \n",
    "    return hidden_state, cell\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Directional LSTM Encoder\n",
    "\n",
    "(https://maxwell.ict.griffith.edu.au/spl/publications/papers/ieeesp97_schuster.pdf)\n",
    "\n",
    "More Info: https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/\n",
    "\n",
    "Bi-directional LSTM encoder has a forward encoder and a backward encoder. The forward encoder encodes a text sequence from start to end, and the backward encoder encodes the text sequence from end to start.\n",
    "The final output is a combination (in this case, a concatenation) of the forward encoded text and the backward encoded text\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = tf.shape(embd_text)[1] #text sequence length\n",
    "N = tf.shape(embd_text)[0] #batch_size\n",
    "\n",
    "i=0\n",
    "hidden=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "cell=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "hidden_forward=tf.TensorArray(size=S, dtype=tf.float32)\n",
    "\n",
    "#shape of embd_text: [N,S,embd_dim]\n",
    "embd_text_t = tf.transpose(embd_text,[1,0,2]) \n",
    "#current shape of embd_text: [S,N,embd_dim]\n",
    "\n",
    "def cond(i, hidden, cell, hidden_forward):\n",
    "    return i < S\n",
    "\n",
    "def body(i, hidden, cell, hidden_forward):\n",
    "    x = embd_text_t[i]\n",
    "    \n",
    "    hidden,cell = LSTM(x,hidden,cell,embd_dim,hidden_size,scope=\"forward_encoder\")\n",
    "    hidden_forward = hidden_forward.write(i, hidden)\n",
    "\n",
    "    return i+1, hidden, cell, hidden_forward\n",
    "\n",
    "_, _, _, hidden_forward = tf.while_loop(cond, body, [i, hidden, cell, hidden_forward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=S-1\n",
    "hidden=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "cell=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "hidden_backward=tf.TensorArray(size=S, dtype=tf.float32)\n",
    "\n",
    "def cond(i, hidden, cell, hidden_backward):\n",
    "    return i >= 0\n",
    "\n",
    "def body(i, hidden, cell, hidden_backward):\n",
    "    x = embd_text_t[i]\n",
    "    hidden,cell = LSTM(x,hidden,cell,embd_dim,hidden_size,scope=\"backward_encoder\")\n",
    "    hidden_backward = hidden_backward.write(i, hidden)\n",
    "\n",
    "    return i-1, hidden, cell, hidden_backward\n",
    "\n",
    "_, _, _, hidden_backward = tf.while_loop(cond, body, [i, hidden, cell, hidden_backward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Forward and Backward Encoder Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_forward = hidden_forward.stack()\n",
    "hidden_backward = hidden_backward.stack()\n",
    "\n",
    "encoder_states = tf.concat([hidden_forward,hidden_backward],axis=-1)\n",
    "encoder_states = tf.transpose(encoder_states,[1,0,2])\n",
    "\n",
    "encoder_states = dropout(encoder_states,rate=0.3,training=tf_train)\n",
    "\n",
    "final_encoded_state = dropout(tf.concat([hidden_forward[-1],hidden_backward[-1]],axis=-1),rate=0.3,training=tf_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of attention scoring function\n",
    "\n",
    "Given a sequence of encoder states ($H_s$) and the decoder hidden state ($H_t$) of current timestep $t$, the equation for computing attention score is:\n",
    "\n",
    "$$Score = (H_s.W_a).H_t^T $$\n",
    "\n",
    "($W_a$ = trainable parameters)\n",
    "\n",
    "(https://nlp.stanford.edu/pubs/emnlp15_attn.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_score(encoder_states,decoder_hidden_state,scope=\"attention_score\"):\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        Wa = tf.get_variable(\"Wa\", shape=[2*hidden_size,2*hidden_size],\n",
    "                                    dtype=tf.float32,\n",
    "                                    trainable=True,\n",
    "                                    initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "    encoder_states = tf.reshape(encoder_states,[N*S,2*hidden_size])\n",
    "    \n",
    "    encoder_states = tf.reshape(tf.matmul(encoder_states,Wa),[N,S,2*hidden_size])\n",
    "    decoder_hidden_state = tf.reshape(decoder_hidden_state,[N,2*hidden_size,1])\n",
    "    \n",
    "    return tf.reshape(tf.matmul(encoder_states,decoder_hidden_state),[N,S])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Attention Function\n",
    "\n",
    "Based on: https://nlp.stanford.edu/pubs/emnlp15_attn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def align(encoder_states, decoder_hidden_state,scope=\"attention\"):\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        Wp = tf.get_variable(\"Wp\", shape=[2*hidden_size,128],\n",
    "                                    dtype=tf.float32,\n",
    "                                    trainable=True,\n",
    "                                    initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "        Vp = tf.get_variable(\"Vp\", shape=[128,1],\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=True,\n",
    "                            initializer=tf.glorot_uniform_initializer())\n",
    "    \n",
    "    positions = tf.cast(S-window_len,dtype=tf.float32) # Maximum valid attention window starting position\n",
    "    \n",
    "    # Predict attention window starting position \n",
    "    ps = positions*tf.nn.sigmoid(tf.matmul(tf.tanh(tf.matmul(decoder_hidden_state,Wp)),Vp))\n",
    "    # ps = (soft-)predicted starting position of attention window\n",
    "    pt = ps+D # pt = center of attention window where the whole window length is 2*D+1\n",
    "    pt = tf.reshape(pt,[N])\n",
    "    \n",
    "    i = 0\n",
    "    gaussian_position_based_scores = tf.TensorArray(size=S,dtype=tf.float32)\n",
    "    sigma = tf.constant(D/2,dtype=tf.float32)\n",
    "    \n",
    "    def cond(i,gaussian_position_based_scores):\n",
    "        \n",
    "        return i < S\n",
    "                      \n",
    "    def body(i,gaussian_position_based_scores):\n",
    "        \n",
    "        score = tf.exp(-((tf.square(tf.cast(i,tf.float32)-pt))/(2*tf.square(sigma)))) \n",
    "        # (equation (10) in https://nlp.stanford.edu/pubs/emnlp15_attn.pdf)\n",
    "        gaussian_position_based_scores = gaussian_position_based_scores.write(i,score)\n",
    "            \n",
    "        return i+1,gaussian_position_based_scores\n",
    "                      \n",
    "    i,gaussian_position_based_scores = tf.while_loop(cond,body,[i,gaussian_position_based_scores])\n",
    "    \n",
    "    gaussian_position_based_scores = gaussian_position_based_scores.stack()\n",
    "    gaussian_position_based_scores = tf.transpose(gaussian_position_based_scores,[1,0])\n",
    "    gaussian_position_based_scores = tf.reshape(gaussian_position_based_scores,[N,S])\n",
    "    \n",
    "    scores = attention_score(encoder_states,decoder_hidden_state)*gaussian_position_based_scores\n",
    "    scores = tf.nn.softmax(scores,axis=-1)\n",
    "    \n",
    "    return tf.reshape(scores,[N,S,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Decoder With Local Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder\",reuse=tf.AUTO_REUSE):\n",
    "    SOS = tf.get_variable(\"sos\", shape=[1,embd_dim],\n",
    "                                dtype=tf.float32,\n",
    "                                trainable=True,\n",
    "                                initializer=tf.glorot_uniform_initializer())\n",
    "    \n",
    "    # SOS represents starting marker \n",
    "    # It tells the decoder that it is about to decode the first word of the output\n",
    "    # I have set SOS as a trainable parameter\n",
    "    \n",
    "    Wc = tf.get_variable(\"Wc\", shape=[4*hidden_size,embd_dim],\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=True,\n",
    "                            initializer=tf.glorot_uniform_initializer())\n",
    "    \n",
    "\n",
    "\n",
    "SOS = tf.tile(SOS,[N,1]) #now SOS shape: [N,embd_dim]\n",
    "inp = SOS\n",
    "hidden=final_encoded_state\n",
    "cell=tf.zeros([N, 2*hidden_size], dtype=tf.float32)\n",
    "decoder_outputs=tf.TensorArray(size=max_summary_len, dtype=tf.float32)\n",
    "outputs=tf.TensorArray(size=max_summary_len, dtype=tf.int32)\n",
    "\n",
    "attention_scores = align(encoder_states,hidden)\n",
    "encoder_context_vector = tf.reduce_sum(encoder_states*attention_scores,axis=1)\n",
    "\n",
    "for i in range(max_summary_len):\n",
    "    \n",
    "    inp = dropout(inp,rate=0.3,training=tf_train)\n",
    "    \n",
    "    inp = tf.concat([inp,encoder_context_vector],axis=-1)\n",
    "    \n",
    "    hidden,cell = LSTM(inp,hidden,cell,embd_dim+2*hidden_size,2*hidden_size,scope=\"decoder\")\n",
    "    \n",
    "    hidden = dropout(hidden,rate=0.3,training=tf_train)\n",
    "    \n",
    "    attention_scores = align(encoder_states,hidden)\n",
    "    encoder_context_vector = tf.reduce_sum(encoder_states*attention_scores,axis=1)\n",
    "    \n",
    "    concated = tf.concat([hidden,encoder_context_vector],axis=-1)\n",
    "    \n",
    "    linear_out = tf.nn.tanh(tf.matmul(concated,Wc))\n",
    "    decoder_output = tf.matmul(linear_out,tf.transpose(tf_embd,[1,0])) \n",
    "    # produce unnormalized probability distribution over vocabulary\n",
    "    \n",
    "    \n",
    "    decoder_outputs = decoder_outputs.write(i,decoder_output)\n",
    "    \n",
    "    # Pick out most probable vocab indices based on the unnormalized probability distribution\n",
    "    \n",
    "    next_word_vec = tf.cast(tf.argmax(decoder_output,1),tf.int32)\n",
    "\n",
    "    next_word_vec = tf.reshape(next_word_vec, [N])\n",
    "\n",
    "    outputs = outputs.write(i,next_word_vec)\n",
    "\n",
    "    next_word = tf.nn.embedding_lookup(tf_embd, next_word_vec)\n",
    "    inp = tf.reshape(next_word, [N, embd_dim])\n",
    "    \n",
    "    \n",
    "decoder_outputs = decoder_outputs.stack()\n",
    "outputs = outputs.stack()\n",
    "\n",
    "decoder_outputs = tf.transpose(decoder_outputs,[1,0,2])\n",
    "outputs = tf.transpose(outputs,[1,0])\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Cross Entropy Cost Function and L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_trainables = [var for var in tf.trainable_variables() if\n",
    "                       not(\"Bias\" in var.name or \"bias\" in var.name\n",
    "                           or \"noreg\" in var.name)]\n",
    "\n",
    "regularization = tf.reduce_sum([tf.nn.l2_loss(var) for var\n",
    "                                in filtered_trainables])\n",
    "\n",
    "with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    epsilon = tf.constant(1e-9, tf.float32)\n",
    "\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=tf_summary, logits=decoder_outputs)\n",
    "\n",
    "    pad_mask = tf.sequence_mask(tf_true_summary_len,\n",
    "                                maxlen=max_summary_len,\n",
    "                                dtype=tf.float32)\n",
    "\n",
    "    masked_cross_entropy = cross_entropy*pad_mask\n",
    "\n",
    "    cost = tf.reduce_mean(masked_cross_entropy) + \\\n",
    "        l2*regularization\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(masked_cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing predicted sequence with labels\n",
    "comparison = tf.cast(tf.equal(outputs, tf_summary),\n",
    "                     tf.float32)\n",
    "\n",
    "# Masking to ignore the effect of pads while calculating accuracy\n",
    "pad_mask = tf.sequence_mask(tf_true_summary_len,\n",
    "                            maxlen=max_summary_len,\n",
    "                            dtype=tf.bool)\n",
    "\n",
    "masked_comparison = tf.boolean_mask(comparison, pad_mask)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = tf.reduce_mean(masked_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "gvs = optimizer.compute_gradients(cost, all_vars)\n",
    "\n",
    "capped_gvs = [(tf.clip_by_norm(grad, 5), var) for grad, var in gvs] # Gradient Clipping\n",
    "\n",
    "train_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Load checkpoint? y/n:  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "STARTING TRAINING\n",
      "\n",
      "\n",
      "Iter 0, Cost= 1.493, Acc = 0.00%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i was given these as a gift ... they were so amazing i now order them for all occasions and sometimes just because i had n't had them in a while . a little warning ; they are completely addictive . i like the <UNK> ones ; my girlfriend likes the rocky road . highly recommended ! < br / > < br / > sure to be appreciated by everyone on your gift list .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "condolence s.e. foodstuff condolence webbed poverty squarely poverty poverty assists foodstuff webbed poverty methodist foodstuff webbed poverty gephardt foodstuff ethier articulos meh rojos cols colombians webbed poverty condolence poverty condolence hourly \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "simply amazing brownies ... \n",
      "\n",
      "\n",
      "Iter 100, Cost= 0.684, Acc = 26.98%\n",
      "Iter 200, Cost= 0.649, Acc = 27.19%\n",
      "Iter 300, Cost= 0.744, Acc = 25.93%\n",
      "Iter 400, Cost= 0.976, Acc = 19.88%\n",
      "Iter 500, Cost= 0.839, Acc = 21.53%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "for those looking for a <UNK> water beverage and one with a neutral taste that does n't have <UNK> aftertaste , this one 's for <UNK> < br / > < br / > also , traditional tap water is slightly more acidic ( i believe ph 7-8 ) . <UNK> 's is supposed at 9.5 ph , so if you 're very sensitive to acidic products , this might help you out .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "good \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "neutral taste , low ph \n",
      "\n",
      "\n",
      "Iter 600, Cost= 0.697, Acc = 27.82%\n",
      "Iter 700, Cost= 0.763, Acc = 24.24%\n",
      "Iter 800, Cost= 0.792, Acc = 24.82%\n",
      "Iter 900, Cost= 0.866, Acc = 23.13%\n",
      "Iter 1000, Cost= 0.838, Acc = 23.03%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i love my starbucks sumatra first thing in the morning . i was not always up early enough to take the detour to starbucks and now i do n't have to ! these <UNK> are perfect and delicious . now i can have my fav coffee even before i take off my slippers ! i love this product ! it 's easy to order - arrived quickly and the price was good .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "no drive through at starbucks ? \n",
      "\n",
      "\n",
      "Iter 1100, Cost= 0.648, Acc = 30.58%\n",
      "Iter 1200, Cost= 0.977, Acc = 19.08%\n",
      "Iter 1300, Cost= 0.788, Acc = 23.29%\n",
      "Iter 1400, Cost= 0.681, Acc = 28.23%\n",
      "Iter 1500, Cost= 0.608, Acc = 29.32%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "husband loves this tea especially in the <UNK> recommend using the large cup setting on your keurig brewer unless you prefer your tea extra strong .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great tea \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "good substitute for coffee . \n",
      "\n",
      "\n",
      "Iter 1600, Cost= 0.709, Acc = 27.48%\n",
      "Iter 1700, Cost= 0.729, Acc = 31.11%\n",
      "Iter 1800, Cost= 0.627, Acc = 28.93%\n",
      "Iter 1900, Cost= 0.798, Acc = 26.36%\n",
      "Iter 2000, Cost= 0.856, Acc = 22.08%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "can no longer find this product locally anymore . i purchased it previously at a warehouse club but costco , bj ` s and sam ` s club no longer stock it in my area stores . my two golden retriever ` s love this gravy when added to their mix of both dry and moist dog food . hope it stays on the market ... <UNK> !\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "best pet food gravy \n",
      "\n",
      "\n",
      "Iter 2100, Cost= 0.640, Acc = 30.77%\n",
      "Iter 2200, Cost= 0.792, Acc = 24.49%\n",
      "Iter 2300, Cost= 0.735, Acc = 22.86%\n",
      "Iter 2400, Cost= 0.769, Acc = 21.68%\n",
      "Iter 2500, Cost= 0.900, Acc = 21.15%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i want to start out by saying that i thought at first that a bag with only 120 calories and 4 grams of fat ( no saturated or trans ) for every 20 chips was going to taste like crap . i must say that not only was i wrong , that this is my favorite bbq chip on the market today . they are light and you can not taste any fat or grease after eating them . that 's because they are n't baked or fried , just popped as their name suggests . these chips are very easy to dip as well . fantastic product !\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great chips \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "fantastic chips ! ! ! \n",
      "\n",
      "\n",
      "Iter 2600, Cost= 0.740, Acc = 22.86%\n",
      "Iter 2700, Cost= 0.848, Acc = 24.84%\n",
      "Iter 2800, Cost= 0.677, Acc = 28.57%\n",
      "Iter 2900, Cost= 0.779, Acc = 25.90%\n",
      "Iter 3000, Cost= 0.718, Acc = 27.34%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "this <UNK> of 7-ounce `` taster 's choice french roast '' canisters , is a good buy . the coffee is flavored differently than original flavor , but the difference is very subtle , and refreshingly good . overall , this taster 's choice coffee is a bargain , and highly recommended .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great flavor \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "good buy \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f62e58d1b154>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m                                              \u001b[0mtf_summary\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_batches_summary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                                              \u001b[0mtf_true_summary_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_batches_true_summary_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                                              tf_train: True})\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mtotal_train_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ML/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ML/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ML/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ML/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ML/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ML/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "with tf.Session() as sess:  # Start Tensorflow Session\n",
    "    display_step = 100\n",
    "    patience = 5\n",
    "\n",
    "    load = input(\"\\nLoad checkpoint? y/n: \")\n",
    "    print(\"\")\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    if load.lower() == 'y':\n",
    "\n",
    "        print('Loading pre-trained weights for the model...')\n",
    "\n",
    "        saver.restore(sess, 'Model_Backup/Seq2seq_summarization.ckpt')\n",
    "        sess.run(tf.global_variables())\n",
    "        sess.run(tf.tables_initializer())\n",
    "\n",
    "        with open('Model_Backup/Seq2seq_summarization.pkl', 'rb') as fp:\n",
    "            train_data = pickle.load(fp)\n",
    "\n",
    "        covered_epochs = train_data['covered_epochs']\n",
    "        best_loss = train_data['best_loss']\n",
    "        impatience = 0\n",
    "        \n",
    "        print('\\nRESTORATION COMPLETE\\n')\n",
    "\n",
    "    else:\n",
    "        best_loss = 2**30\n",
    "        impatience = 0\n",
    "        covered_epochs = 0\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        sess.run(tf.tables_initializer())\n",
    "\n",
    "    epoch=0\n",
    "    while (epoch+covered_epochs)<epochs:\n",
    "        \n",
    "        print(\"\\n\\nSTARTING TRAINING\\n\\n\")\n",
    "        \n",
    "        batches_indices = [i for i in range(0, len(train_batches_text))]\n",
    "        random.shuffle(batches_indices)\n",
    "\n",
    "        total_train_acc = 0\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for i in range(0, len(train_batches_text)):\n",
    "            \n",
    "            j = int(batches_indices[i])\n",
    "\n",
    "            cost,prediction,\\\n",
    "                acc, _ = sess.run([cross_entropy,\n",
    "                                   outputs,\n",
    "                                   accuracy,\n",
    "                                   train_op],\n",
    "                                  feed_dict={tf_text: train_batches_text[j],\n",
    "                                             tf_embd: embd,\n",
    "                                             tf_summary: train_batches_summary[j],\n",
    "                                             tf_true_summary_len: train_batches_true_summary_len[j],\n",
    "                                             tf_train: True})\n",
    "            \n",
    "            total_train_acc += acc\n",
    "            total_train_loss += cost\n",
    "\n",
    "            if i % display_step == 0:\n",
    "                print(\"Iter \"+str(i)+\", Cost= \" +\n",
    "                      \"{:.3f}\".format(cost)+\", Acc = \" +\n",
    "                      \"{:.2f}%\".format(acc*100))\n",
    "            \n",
    "            if i % 500 == 0:\n",
    "                \n",
    "                idx = random.randint(0,len(train_batches_text[j])-1)\n",
    "                \n",
    "                \n",
    "                \n",
    "                text = \" \".join([idx2vocab.get(vec,\"<UNK>\") for vec in train_batches_text[j][idx]])\n",
    "                predicted_summary = [idx2vocab.get(vec,\"<UNK>\") for vec in prediction[idx]]\n",
    "                actual_summary = [idx2vocab.get(vec,\"<UNK>\") for vec in train_batches_summary[j][idx]]\n",
    "                \n",
    "                print(\"\\nSample Text\\n\")\n",
    "                print(text)\n",
    "                print(\"\\nSample Predicted Summary\\n\")\n",
    "                for word in predicted_summary:\n",
    "                    if word == '<EOS>':\n",
    "                        break\n",
    "                    else:\n",
    "                        print(word,end=\" \")\n",
    "                print(\"\\n\\nSample Actual Summary\\n\")\n",
    "                for word in actual_summary:\n",
    "                    if word == '<EOS>':\n",
    "                        break\n",
    "                    else:\n",
    "                        print(word,end=\" \")\n",
    "                print(\"\\n\\n\")\n",
    "                \n",
    "        print(\"\\n\\nSTARTING VALIDATION\\n\\n\")\n",
    "                \n",
    "        total_val_loss=0\n",
    "        total_val_acc=0\n",
    "                \n",
    "        for i in range(0, len(val_batches_text)):\n",
    "            \n",
    "            if i%100==0:\n",
    "                print(\"Validating data # {}\".format(i))\n",
    "\n",
    "            cost, prediction,\\\n",
    "                acc = sess.run([cross_entropy,\n",
    "                                outputs,\n",
    "                                accuracy],\n",
    "                                  feed_dict={tf_text: val_batches_text[i],\n",
    "                                             tf_embd: embd,\n",
    "                                             tf_summary: val_batches_summary[i],\n",
    "                                             tf_true_summary_len: val_batches_true_summary_len[i],\n",
    "                                             tf_train: False})\n",
    "            \n",
    "            total_val_loss += cost\n",
    "            total_val_acc += acc\n",
    "            \n",
    "        avg_val_loss = total_val_loss/len(val_batches_text)\n",
    "        \n",
    "        print(\"\\n\\nEpoch: {}\\n\\n\".format(epoch+covered_epochs))\n",
    "        print(\"Average Training Loss: {:.3f}\".format(total_train_loss/len(train_batches_text)))\n",
    "        print(\"Average Training Accuracy: {:.2f}\".format(100*total_train_acc/len(train_batches_text)))\n",
    "        print(\"Average Validation Loss: {:.3f}\".format(avg_val_loss))\n",
    "        print(\"Average Validation Accuracy: {:.2f}\".format(100*total_val_acc/len(val_batches_text)))\n",
    "              \n",
    "        if (avg_val_loss < best_loss):\n",
    "            best_loss = avg_val_loss\n",
    "            save_data={'best_loss':best_loss,'covered_epochs':covered_epochs+epoch+1}\n",
    "            impatience=0\n",
    "            with open('Model_Backup/Seq2seq_summarization.pkl', 'wb') as fp:\n",
    "                pickle.dump(save_data, fp)\n",
    "            saver.save(sess, 'Model_Backup/Seq2seq_summarization.ckpt')\n",
    "            print(\"\\nModel saved\\n\")\n",
    "              \n",
    "        else:\n",
    "            impatience+=1\n",
    "              \n",
    "        if impatience > patience:\n",
    "              break\n",
    "              \n",
    "              \n",
    "        epoch+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Works\n",
    "\n",
    "* Beam Search\n",
    "* Pointer Mechanisms\n",
    "* BLEU\\ROUGE evaluation\n",
    "* Implement Testing\n",
    "* Complete Training and Optimize Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
