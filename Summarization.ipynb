{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pre-processed Dataset\n",
    "\n",
    "The Data is preprocessed in [Data_Pre-Processing.ipynb](https://github.com/JRC1995/Abstractive-Summarization/blob/master/Data_Pre-Processing.ipynb)\n",
    "\n",
    "Dataset source: https://www.kaggle.com/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('Processed_Data/Amazon_Reviews_Processed.json') as file:\n",
    "\n",
    "    for json_data in file:\n",
    "        saved_data = json.loads(json_data)\n",
    "\n",
    "        vocab2idx = saved_data[\"vocab\"]\n",
    "        embd = saved_data[\"embd\"]\n",
    "        train_batches_text = saved_data[\"train_batches_text\"]\n",
    "        test_batches_text = saved_data[\"test_batches_text\"]\n",
    "        val_batches_text = saved_data[\"val_batches_text\"]\n",
    "        train_batches_summary = saved_data[\"train_batches_summary\"]\n",
    "        test_batches_summary = saved_data[\"test_batches_summary\"]\n",
    "        val_batches_summary = saved_data[\"val_batches_summary\"]\n",
    "        train_batches_true_text_len = saved_data[\"train_batches_true_text_len\"]\n",
    "        val_batches_true_text_len = saved_data[\"val_batches_true_text_len\"]\n",
    "        test_batches_true_text_len = saved_data[\"test_batches_true_text_len\"]\n",
    "        train_batches_true_summary_len = saved_data[\"train_batches_true_summary_len\"]\n",
    "        val_batches_true_summary_len = saved_data[\"val_batches_true_summary_len\"]\n",
    "        test_batches_true_summary_len = saved_data[\"test_batches_true_summary_len\"]\n",
    "\n",
    "        break\n",
    "        \n",
    "idx2vocab = {v:k for k,v in vocab2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "max_summary_len = 16\n",
    "D = 5 # D determines local attention window size\n",
    "window_len = 2*D+1\n",
    "l2=1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "embd_dim = len(embd[0])\n",
    "\n",
    "tf_text = tf.placeholder(tf.int32, [None, None])\n",
    "tf_embd = tf.placeholder(tf.float32, [len(vocab2idx),embd_dim])\n",
    "tf_true_summary_len = tf.placeholder(tf.int32, [None])\n",
    "tf_summary = tf.placeholder(tf.int32,[None, None])\n",
    "tf_train = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed vectorized text\n",
    "\n",
    "Dropout used for regularization \n",
    "(https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_text = tf.nn.embedding_lookup(tf_embd, tf_text)\n",
    "embd_text = tf.layers.dropout(embd_text,rate=0.3,training=tf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM function\n",
    "\n",
    "More info: \n",
    "<br>\n",
    "https://dl.acm.org/citation.cfm?id=1246450, \n",
    "<br>\n",
    "https://www.bioinf.jku.at/publications/older/2604.pdf,\n",
    "<br>\n",
    "https://en.wikipedia.org/wiki/Long_short-term_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(x,hidden_state,cell,input_dim,hidden_size,scope):\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        w = tf.get_variable(\"w\", shape=[4,input_dim,hidden_size],\n",
    "                                    dtype=tf.float32,\n",
    "                                    trainable=True,\n",
    "                                    initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "        u = tf.get_variable(\"u\", shape=[4,hidden_size,hidden_size],\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=True,\n",
    "                            initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "        b = tf.get_variable(\"bias\", shape=[4,1,hidden_size],\n",
    "                    dtype=tf.float32,\n",
    "                    trainable=True,\n",
    "                    initializer=tf.zeros_initializer())\n",
    "        \n",
    "    input_gate = tf.nn.sigmoid( tf.matmul(x,w[0]) + tf.matmul(hidden_state,u[0]) + b[0])\n",
    "    forget_gate = tf.nn.sigmoid( tf.matmul(x,w[1]) + tf.matmul(hidden_state,u[1]) + b[1])\n",
    "    output_gate = tf.nn.sigmoid( tf.matmul(x,w[2]) + tf.matmul(hidden_state,u[2]) + b[2])\n",
    "    cell_ = tf.nn.tanh( tf.matmul(x,w[3]) + tf.matmul(hidden_state,u[3]) + b[3])\n",
    "    cell = forget_gate*cell + input_gate*cell_\n",
    "    hidden_state = output_gate*tf.tanh(cell)\n",
    "    \n",
    "    return hidden_state, cell\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Directional LSTM Encoder\n",
    "\n",
    "(https://maxwell.ict.griffith.edu.au/spl/publications/papers/ieeesp97_schuster.pdf)\n",
    "\n",
    "More Info: https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/\n",
    "\n",
    "Bi-directional LSTM encoder has a forward encoder and a backward encoder. The forward encoder encodes a text sequence from start to end, and the backward encoder encodes the text sequence from end to start.\n",
    "The final output is a combination (in this case, a concatenation) of the forward encoded text and the backward encoded text\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = tf.shape(embd_text)[1] #text sequence length\n",
    "N = tf.shape(embd_text)[0] #batch_size\n",
    "\n",
    "i=0\n",
    "hidden=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "cell=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "hidden_forward=tf.TensorArray(size=S, dtype=tf.float32)\n",
    "\n",
    "#shape of embd_text: [N,S,embd_dim]\n",
    "embd_text_t = tf.transpose(embd_text,[1,0,2]) \n",
    "#current shape of embd_text: [S,N,embd_dim]\n",
    "\n",
    "def cond(i, hidden, cell, hidden_forward):\n",
    "    return i < S\n",
    "\n",
    "def body(i, hidden, cell, hidden_forward):\n",
    "    x = embd_text_t[i]\n",
    "    \n",
    "    hidden,cell = LSTM(x,hidden,cell,embd_dim,hidden_size,scope=\"forward_encoder\")\n",
    "    hidden_forward = hidden_forward.write(i, hidden)\n",
    "\n",
    "    return i+1, hidden, cell, hidden_forward\n",
    "\n",
    "_, _, _, hidden_forward = tf.while_loop(cond, body, [i, hidden, cell, hidden_forward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=S-1\n",
    "hidden=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "cell=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "hidden_backward=tf.TensorArray(size=S, dtype=tf.float32)\n",
    "\n",
    "def cond(i, hidden, cell, hidden_backward):\n",
    "    return i >= 0\n",
    "\n",
    "def body(i, hidden, cell, hidden_backward):\n",
    "    x = embd_text_t[i]\n",
    "    hidden,cell = LSTM(x,hidden,cell,embd_dim,hidden_size,scope=\"backward_encoder\")\n",
    "    hidden_backward = hidden_backward.write(i, hidden)\n",
    "\n",
    "    return i-1, hidden, cell, hidden_backward\n",
    "\n",
    "_, _, _, hidden_backward = tf.while_loop(cond, body, [i, hidden, cell, hidden_backward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Forward and Backward Encoder Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_forward = hidden_forward.stack()\n",
    "hidden_backward = hidden_backward.stack()\n",
    "hidden_backward_rev = tf.reverse(hidden_backward,axis=[1])\n",
    "\n",
    "encoder_states = tf.concat([hidden_forward,hidden_backward_rev],axis=-1)\n",
    "encoder_states = tf.transpose(encoder_states,[1,0,2])\n",
    "\n",
    "encoder_states = tf.layers.dropout(encoder_states,rate=0.3,training=tf_train)\n",
    "\n",
    "final_encoded_state = tf.layers.dropout(tf.concat([hidden_forward[-1],hidden_backward[-1]],axis=-1),rate=0.3,training=tf_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of attention scoring function\n",
    "\n",
    "Given a sequence of encoder states ($H_s$) and the decoder hidden state ($H_t$) of current timestep $t$, the equation for computing attention score is:\n",
    "\n",
    "$$Score = (H_s.W_a).H_t^T $$\n",
    "\n",
    "($W_a$ = trainable parameters)\n",
    "\n",
    "(https://nlp.stanford.edu/pubs/emnlp15_attn.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_score(encoder_states,decoder_hidden_state,scope=\"attention_score\"):\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        Wa = tf.get_variable(\"Wa\", shape=[2*hidden_size,2*hidden_size],\n",
    "                                    dtype=tf.float32,\n",
    "                                    trainable=True,\n",
    "                                    initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "    encoder_states = tf.reshape(encoder_states,[N*S,2*hidden_size])\n",
    "    \n",
    "    encoder_states = tf.reshape(tf.matmul(encoder_states,Wa),[N,S,2*hidden_size])\n",
    "    decoder_hidden_state = tf.reshape(decoder_hidden_state,[N,2*hidden_size,1])\n",
    "    \n",
    "    return tf.reshape(tf.matmul(encoder_states,decoder_hidden_state),[N,S])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Attention Function\n",
    "\n",
    "Based on: https://nlp.stanford.edu/pubs/emnlp15_attn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def align(encoder_states, decoder_hidden_state,scope=\"attention\"):\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        Wp = tf.get_variable(\"Wp\", shape=[2*hidden_size,125],\n",
    "                                    dtype=tf.float32,\n",
    "                                    trainable=True,\n",
    "                                    initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "        Vp = tf.get_variable(\"Vp\", shape=[125,1],\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=True,\n",
    "                            initializer=tf.glorot_uniform_initializer())\n",
    "    \n",
    "    positions = tf.cast(S-window_len,dtype=tf.float32) # Maximum valid attention window starting position\n",
    "    \n",
    "    # Predict attention window starting position \n",
    "    ps = positions*tf.nn.sigmoid(tf.matmul(tf.tanh(tf.matmul(decoder_hidden_state,Wp)),Vp))\n",
    "    # ps = (soft-)predicted starting position of attention window\n",
    "    pt = ps+D # pt = center of attention window where the whole window length is 2*D+1\n",
    "    pt = tf.reshape(pt,[N])\n",
    "    \n",
    "    i = 0\n",
    "    gaussian_position_based_scores = tf.TensorArray(size=S,dtype=tf.float32)\n",
    "    sigma = tf.constant(D/2,dtype=tf.float32)\n",
    "    \n",
    "    def cond(i,gaussian_position_based_scores):\n",
    "        \n",
    "        return i < S\n",
    "                      \n",
    "    def body(i,gaussian_position_based_scores):\n",
    "        \n",
    "        score = tf.exp(-((tf.square(tf.cast(i,tf.float32)-pt))/(2*tf.square(sigma)))) \n",
    "        # (equation (10) in https://nlp.stanford.edu/pubs/emnlp15_attn.pdf)\n",
    "        gaussian_position_based_scores = gaussian_position_based_scores.write(i,score)\n",
    "            \n",
    "        return i+1,gaussian_position_based_scores\n",
    "                      \n",
    "    i,gaussian_position_based_scores = tf.while_loop(cond,body,[i,gaussian_position_based_scores])\n",
    "    \n",
    "    gaussian_position_based_scores = gaussian_position_based_scores.stack()\n",
    "    gaussian_position_based_scores = tf.transpose(gaussian_position_based_scores,[1,0])\n",
    "    gaussian_position_based_scores = tf.reshape(gaussian_position_based_scores,[N,S])\n",
    "    \n",
    "    scores = attention_score(encoder_states,decoder_hidden_state)*gaussian_position_based_scores\n",
    "    scores = tf.nn.softmax(scores,axis=-1)\n",
    "    \n",
    "    return tf.reshape(scores,[N,S,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Decoder With Local Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder\",reuse=tf.AUTO_REUSE):\n",
    "    SOS = tf.get_variable(\"sos\", shape=[1,embd_dim],\n",
    "                                dtype=tf.float32,\n",
    "                                trainable=True,\n",
    "                                initializer=tf.glorot_uniform_initializer())\n",
    "    \n",
    "    # SOS represents starting marker \n",
    "    # It tells the decoder that it is about to decode the first word of the output\n",
    "    # I have set SOS as a trainable parameter\n",
    "    \n",
    "    Wc = tf.get_variable(\"Wc\", shape=[4*hidden_size,embd_dim],\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=True,\n",
    "                            initializer=tf.glorot_uniform_initializer())\n",
    "    \n",
    "\n",
    "\n",
    "SOS = tf.tile(SOS,[N,1]) #now SOS shape: [N,embd_dim]\n",
    "inp = SOS\n",
    "hidden=final_encoded_state\n",
    "cell=tf.zeros([N, 2*hidden_size], dtype=tf.float32)\n",
    "decoder_outputs=tf.TensorArray(size=max_summary_len, dtype=tf.float32)\n",
    "outputs=tf.TensorArray(size=max_summary_len, dtype=tf.int32)\n",
    "\n",
    "for i in range(max_summary_len):\n",
    "    \n",
    "    inp = tf.layers.dropout(inp,rate=0.3,training=tf_train)\n",
    "    \n",
    "    attention_scores = align(encoder_states,hidden)\n",
    "    encoder_context_vector = tf.reduce_sum(encoder_states*attention_scores,axis=1)\n",
    "    \n",
    "    hidden,cell = LSTM(inp,hidden,cell,embd_dim,2*hidden_size,scope=\"decoder\")\n",
    "    \n",
    "    hidden_ = tf.layers.dropout(hidden,rate=0.3,training=tf_train)\n",
    "    \n",
    "    concated = tf.concat([hidden_,encoder_context_vector],axis=-1)\n",
    "    \n",
    "    linear_out = tf.nn.tanh(tf.matmul(concated,Wc))\n",
    "    decoder_output = tf.matmul(linear_out,tf.transpose(tf_embd,[1,0])) \n",
    "    # produce unnormalized probability distribution over vocabulary\n",
    "    \n",
    "    \n",
    "    decoder_outputs = decoder_outputs.write(i,decoder_output)\n",
    "    \n",
    "    # Pick out most probable vocab indices based on the unnormalized probability distribution\n",
    "    \n",
    "    next_word_vec = tf.cast(tf.argmax(decoder_output,1),tf.int32)\n",
    "\n",
    "    next_word_vec = tf.reshape(next_word_vec, [N])\n",
    "\n",
    "    outputs = outputs.write(i,next_word_vec)\n",
    "\n",
    "    next_word = tf.nn.embedding_lookup(tf_embd, next_word_vec)\n",
    "    inp = tf.reshape(next_word, [N, embd_dim])\n",
    "    \n",
    "    \n",
    "decoder_outputs = decoder_outputs.stack()\n",
    "outputs = outputs.stack()\n",
    "\n",
    "decoder_outputs = tf.transpose(decoder_outputs,[1,0,2])\n",
    "outputs = tf.transpose(outputs,[1,0])\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Cross Entropy Cost Function and L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_trainables = [var for var in tf.trainable_variables() if\n",
    "                       not(\"Bias\" in var.name or \"bias\" in var.name\n",
    "                           or \"noreg\" in var.name)]\n",
    "\n",
    "regularization = tf.reduce_sum([tf.nn.l2_loss(var) for var\n",
    "                                in filtered_trainables])\n",
    "\n",
    "with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    epsilon = tf.constant(1e-9, tf.float32)\n",
    "\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=tf_summary, logits=decoder_outputs)\n",
    "\n",
    "    pad_mask = tf.sequence_mask(tf_true_summary_len,\n",
    "                                maxlen=max_summary_len,\n",
    "                                dtype=tf.float32)\n",
    "\n",
    "    masked_cross_entropy = cross_entropy*pad_mask\n",
    "\n",
    "    cost = tf.reduce_mean(masked_cross_entropy) + \\\n",
    "        l2*regularization\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(masked_cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing predicted sequence with labels\n",
    "comparison = tf.cast(tf.equal(outputs, tf_summary),\n",
    "                     tf.float32)\n",
    "\n",
    "# Masking to ignore the effect of pads while calculating accuracy\n",
    "pad_mask = tf.sequence_mask(tf_true_summary_len,\n",
    "                            maxlen=max_summary_len,\n",
    "                            dtype=tf.bool)\n",
    "\n",
    "masked_comparison = tf.boolean_mask(comparison, pad_mask)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = tf.reduce_mean(masked_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "optimizer = tf.contrib.opt.NadamOptimizer(\n",
    "    learning_rate=learning_rate)\n",
    "\n",
    "gvs = optimizer.compute_gradients(cost, var_list=all_vars)\n",
    "\n",
    "capped_gvs = [(tf.clip_by_norm(grad, 5), var) for grad, var in gvs] # Gradient Clipping\n",
    "\n",
    "train_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load checkpoint? y/n: n\n",
      "\n",
      "\n",
      "\n",
      "STARTING TRAINING\n",
      "\n",
      "\n",
      "Iter 0, Cost= 2.086, Acc = 0.00%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i 'm not a big pretzel eater , but i love this little <UNK> nibblers . i like the low fat snack and how it fills you up .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "municipality jackass municipality mongolian seats han han mongolian hah sus sus wat hah casbah dynasty province \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "great pretzels \n",
      "\n",
      "\n",
      "Iter 100, Cost= 0.985, Acc = 35.58%\n",
      "Iter 200, Cost= 0.914, Acc = 33.33%\n",
      "Iter 300, Cost= 0.928, Acc = 36.11%\n",
      "Iter 400, Cost= 0.943, Acc = 35.19%\n",
      "Iter 500, Cost= 0.676, Acc = 42.71%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "we <UNK> this one , but the flavor could have been a tad stronger . very yummy tho , we will totally purchase again !\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "delicious ! \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "very good ! \n",
      "\n",
      "\n",
      "Iter 600, Cost= 0.878, Acc = 35.24%\n",
      "Iter 700, Cost= 0.949, Acc = 33.04%\n",
      "Iter 800, Cost= 1.074, Acc = 34.65%\n",
      "Iter 900, Cost= 0.831, Acc = 44.21%\n",
      "Iter 1000, Cost= 0.911, Acc = 36.36%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "tried this hoping for something better than the thick salsa that everyone else makes and it was great ! after making our own it gets time consuming so this is a good alternative .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "great salsa \n",
      "\n",
      "\n",
      "Iter 1100, Cost= 1.081, Acc = 23.33%\n",
      "Iter 1200, Cost= 1.018, Acc = 32.73%\n",
      "Iter 1300, Cost= 0.902, Acc = 35.87%\n",
      "Iter 1400, Cost= 0.946, Acc = 31.07%\n",
      "Iter 1500, Cost= 0.798, Acc = 42.31%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i had a coupon for this so it was a good value . otherwise it is to expense for what you get . my box had a couple of opened cereals in it so i did n't get the full value of all ...\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "good \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "good value \n",
      "\n",
      "\n",
      "Iter 1600, Cost= 0.871, Acc = 33.33%\n",
      "Iter 1700, Cost= 0.943, Acc = 40.00%\n",
      "Iter 1800, Cost= 0.876, Acc = 40.20%\n",
      "Iter 1900, Cost= 0.973, Acc = 37.25%\n",
      "Iter 2000, Cost= 0.978, Acc = 29.73%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "my 4 dogs all had allergies and are just fine now that i switched to <UNK> the <UNK> one smell abit but <UNK> they still love it <UNK> the dried <UNK> canned r terrific <UNK> nooo grani !\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "<UNK> ! \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "great food \n",
      "\n",
      "\n",
      "Iter 2100, Cost= 0.907, Acc = 37.04%\n",
      "Iter 2200, Cost= 0.928, Acc = 34.31%\n",
      "Iter 2300, Cost= 0.906, Acc = 31.25%\n",
      "Iter 2400, Cost= 0.903, Acc = 37.00%\n",
      "Iter 2500, Cost= 0.811, Acc = 33.01%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "the chocolate was a little crumbly , but the taste is very good . my hubby has <UNK> , and it is gluten free , so it is an excellent bar to stock in the pantry for whenever he does n't have time for breakfast .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "yum \n",
      "\n",
      "\n",
      "Iter 2600, Cost= 0.839, Acc = 34.62%\n",
      "Iter 2700, Cost= 0.927, Acc = 37.07%\n",
      "Iter 2800, Cost= 0.853, Acc = 36.73%\n",
      "Iter 2900, Cost= 0.805, Acc = 40.00%\n",
      "Iter 3000, Cost= 0.855, Acc = 35.51%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "tea came packaged as expected , delivered quickly and with stash you can not go wrong . individually wrapped and stays fresh and very flavorful . highly recommended for the earl gray tea lover .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "delicious tea \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "great tea \n",
      "\n",
      "\n",
      "Iter 3100, Cost= 0.854, Acc = 36.63%\n",
      "\n",
      "\n",
      "STARTING VALIDATION\n",
      "\n",
      "\n",
      "Validating data # 0\n",
      "Validating data # 100\n",
      "Validating data # 200\n",
      "Validating data # 300\n",
      "\n",
      "\n",
      "Epoch: 0\n",
      "\n",
      "\n",
      "Average Training Loss: 0.907\n",
      "Average Training Accuracy: 35.42\n",
      "Average Validation Loss: 0.865\n",
      "Average Validation Accuracy: 36.65\n",
      "\n",
      "Model saved\n",
      "\n",
      "\n",
      "\n",
      "STARTING TRAINING\n",
      "\n",
      "\n",
      "Iter 0, Cost= 0.808, Acc = 34.34%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "quaker oatmeal squares has been our family favorite for a couple of years now . ca n't get enough of it . just the right sweetness and crunch .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "favorite cereal \n",
      "\n",
      "\n",
      "Iter 100, Cost= 1.036, Acc = 34.26%\n",
      "Iter 200, Cost= 0.934, Acc = 33.03%\n",
      "Iter 300, Cost= 0.972, Acc = 35.85%\n",
      "Iter 400, Cost= 0.926, Acc = 32.35%\n",
      "Iter 500, Cost= 0.738, Acc = 41.05%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "great taste , nice smell , great <UNK> < br / > if you mix it with fresh ment you will get fantastic <UNK> < br / > i will buy it again .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "the best \n",
      "\n",
      "\n",
      "Iter 600, Cost= 0.858, Acc = 41.24%\n",
      "Iter 700, Cost= 0.905, Acc = 36.45%\n",
      "Iter 800, Cost= 0.795, Acc = 35.05%\n",
      "Iter 900, Cost= 0.806, Acc = 37.50%\n",
      "Iter 1000, Cost= 0.795, Acc = 35.64%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i bought about 5 different kinds of <UNK> when i first got my coffee maker , which i love by the way , and i 'd have to say that this was my favorite one out of them all . it has the perfect balance of everything , i was really surprised .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "excellent stuff \n",
      "\n",
      "\n",
      "Iter 1100, Cost= 0.825, Acc = 39.42%\n",
      "Iter 1200, Cost= 0.743, Acc = 38.78%\n",
      "Iter 1300, Cost= 0.813, Acc = 41.84%\n",
      "Iter 1400, Cost= 0.933, Acc = 29.66%\n",
      "Iter 1500, Cost= 0.978, Acc = 33.61%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i really wanted to like this , as it was organic , and came in a glass bottle , but there was hardly any flavor at all . i could barely smell it , and even when i poured a generous amount on my dish , it imparts little to no truffle <UNK> . my truffle salt is much more potent .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "good \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "weak \n",
      "\n",
      "\n",
      "Iter 1600, Cost= 0.778, Acc = 45.10%\n",
      "Iter 1700, Cost= 0.855, Acc = 38.83%\n",
      "Iter 1800, Cost= 0.815, Acc = 41.58%\n",
      "Iter 1900, Cost= 0.853, Acc = 37.62%\n",
      "Iter 2000, Cost= 1.003, Acc = 32.74%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i love milk chocolate and do n't like dark <UNK> . my husband is the opposite , so i always buy him the dark stuff and it 's safe for him , haha ! until i happened to try this one . it 's awesome !\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "<UNK> ! \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "it 's good ! ! \n",
      "\n",
      "\n",
      "Iter 2100, Cost= 0.817, Acc = 37.74%\n",
      "Iter 2200, Cost= 0.977, Acc = 33.33%\n",
      "Iter 2300, Cost= 0.840, Acc = 35.96%\n",
      "Iter 2400, Cost= 0.749, Acc = 31.58%\n",
      "Iter 2500, Cost= 0.885, Acc = 31.73%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "the best thing about this coffee is the sweet smell , just like a blueberry muffin . the taste is good , not as sweet as i was expecting but it was good nonetheless . its a nice treat when you 're craving something sweet but it wo n't replace my morning donut shop coffee : )\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "delicious \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "smells yummy : ) \n",
      "\n",
      "\n",
      "Iter 2600, Cost= 0.887, Acc = 32.73%\n",
      "Iter 2700, Cost= 0.780, Acc = 44.94%\n",
      "Iter 2800, Cost= 0.899, Acc = 35.71%\n",
      "Iter 2900, Cost= 0.797, Acc = 38.24%\n",
      "Iter 3000, Cost= 1.061, Acc = 33.33%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "this tea is wonderful , one bag will make three cups for most people . i like my tea very strong so these were perfect . i bet they will be good for making a good ice tea .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great tea \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "one bag 3 cups \n",
      "\n",
      "\n",
      "Iter 3100, Cost= 0.769, Acc = 37.86%\n",
      "\n",
      "\n",
      "STARTING VALIDATION\n",
      "\n",
      "\n",
      "Validating data # 0\n",
      "Validating data # 100\n",
      "Validating data # 200\n",
      "Validating data # 300\n",
      "\n",
      "\n",
      "Epoch: 1\n",
      "\n",
      "\n",
      "Average Training Loss: 0.863\n",
      "Average Training Accuracy: 36.40\n",
      "Average Validation Loss: 0.837\n",
      "Average Validation Accuracy: 37.30\n",
      "\n",
      "Model saved\n",
      "\n",
      "\n",
      "\n",
      "STARTING TRAINING\n",
      "\n",
      "\n",
      "Iter 0, Cost= 0.959, Acc = 35.85%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "really good bars . you could cut this baby in 1/2 and have 2 snacks out of it ! i bought 1 at the store first to see if i liked them and paid lots more for it . i do n't eat alot of meat so this caught my eye . i now have them on auto delivery ! !\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great ! \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "great bars ! \n",
      "\n",
      "\n",
      "Iter 100, Cost= 0.792, Acc = 33.33%\n",
      "Iter 200, Cost= 0.781, Acc = 35.29%\n",
      "Iter 300, Cost= 0.825, Acc = 40.74%\n",
      "Iter 400, Cost= 0.793, Acc = 40.19%\n",
      "Iter 500, Cost= 0.860, Acc = 31.07%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i always buy my coffee from amazon as the prices are cheaper and i love all the coffee . best price on line .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great coffee \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "peggy \n",
      "\n",
      "\n",
      "Iter 600, Cost= 0.990, Acc = 28.57%\n",
      "Iter 700, Cost= 0.736, Acc = 41.41%\n",
      "Iter 800, Cost= 0.826, Acc = 33.68%\n",
      "Iter 900, Cost= 0.904, Acc = 35.24%\n",
      "Iter 1000, Cost= 0.858, Acc = 35.71%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i am very pleased with this product and the company sent it on a timely basis , well packed to prevent breakage .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "good stuff \n",
      "\n",
      "\n",
      "Iter 1100, Cost= 0.999, Acc = 30.36%\n",
      "Iter 1200, Cost= 0.726, Acc = 44.79%\n",
      "Iter 1300, Cost= 0.798, Acc = 36.73%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1400, Cost= 0.712, Acc = 48.98%\n",
      "Iter 1500, Cost= 0.884, Acc = 38.18%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i love this cereal , easy to eat out of the box . sweet but not too sweet and very crunchy . since frosted mini wheats have gelatin in them they are not an option for vegans or vegetarians , these are the best replacement .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "<UNK> \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "love it . \n",
      "\n",
      "\n",
      "Iter 1600, Cost= 0.819, Acc = 44.12%\n",
      "Iter 1700, Cost= 0.958, Acc = 32.11%\n",
      "Iter 1800, Cost= 0.800, Acc = 37.96%\n",
      "Iter 1900, Cost= 0.649, Acc = 40.82%\n",
      "Iter 2000, Cost= 0.900, Acc = 36.21%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i have been buying these bars ( without chocolate ) for years and have recently found them with the chocolate . our family of six loves them . they are a great bar to hold you over and give you extended fuel , made with great ingredients to boot . i highly recommend you try a box !\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great ! \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "<UNK> ! \n",
      "\n",
      "\n",
      "Iter 2100, Cost= 0.767, Acc = 36.19%\n",
      "Iter 2200, Cost= 0.676, Acc = 37.62%\n",
      "Iter 2300, Cost= 0.871, Acc = 40.00%\n",
      "Iter 2400, Cost= 0.687, Acc = 39.60%\n",
      "Iter 2500, Cost= 0.898, Acc = 36.27%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "this is top notch almond syrup . we put it in lemonade and strawberries . great for many <UNK> < br / > also use in baking recipes .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "soooo yummy \n",
      "\n",
      "\n",
      "Iter 2600, Cost= 0.796, Acc = 40.74%\n",
      "Iter 2700, Cost= 0.775, Acc = 46.00%\n",
      "Iter 2800, Cost= 0.856, Acc = 40.19%\n",
      "Iter 2900, Cost= 0.954, Acc = 35.85%\n",
      "Iter 3000, Cost= 0.831, Acc = 35.40%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "this is very good coffee at a good price ... it is an old product that has been on the market since i was quite young .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "coffee \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "good stuff ! \n",
      "\n",
      "\n",
      "Iter 3100, Cost= 0.754, Acc = 36.27%\n",
      "\n",
      "\n",
      "STARTING VALIDATION\n",
      "\n",
      "\n",
      "Validating data # 0\n",
      "Validating data # 100\n",
      "Validating data # 200\n",
      "Validating data # 300\n",
      "\n",
      "\n",
      "Epoch: 2\n",
      "\n",
      "\n",
      "Average Training Loss: 0.840\n",
      "Average Training Accuracy: 37.26\n",
      "Average Validation Loss: 0.818\n",
      "Average Validation Accuracy: 38.42\n",
      "\n",
      "Model saved\n",
      "\n",
      "\n",
      "\n",
      "STARTING TRAINING\n",
      "\n",
      "\n",
      "Iter 0, Cost= 0.822, Acc = 36.36%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "the chocolate covered figs were delicious and presented beautifully in the package . great for a gift for someone who has everything .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "delicious \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "figs \n",
      "\n",
      "\n",
      "Iter 100, Cost= 0.734, Acc = 37.86%\n",
      "Iter 200, Cost= 0.837, Acc = 41.18%\n",
      "Iter 300, Cost= 0.717, Acc = 34.91%\n",
      "Iter 400, Cost= 0.797, Acc = 38.61%\n",
      "Iter 500, Cost= 0.718, Acc = 32.38%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "one of my favorite flavors of <UNK> , it used to be called twisted tornado , now called fruit twist either way i ca n't stop myself from eating its so flavorful = )\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great ! \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "yum \n",
      "\n",
      "\n",
      "Iter 600, Cost= 0.846, Acc = 40.59%\n",
      "Iter 700, Cost= 0.676, Acc = 43.75%\n",
      "Iter 800, Cost= 0.882, Acc = 39.22%\n",
      "Iter 900, Cost= 0.803, Acc = 36.54%\n",
      "Iter 1000, Cost= 0.718, Acc = 40.40%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i found this product to be a nice tasting pepper blend and would recommend it to all of those who enjoy the fresh flavor of ground pepper .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "good taste \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "peppercorn mix \n",
      "\n",
      "\n",
      "Iter 1100, Cost= 0.749, Acc = 41.24%\n",
      "Iter 1200, Cost= 0.821, Acc = 38.10%\n",
      "Iter 1300, Cost= 0.883, Acc = 39.81%\n",
      "Iter 1400, Cost= 0.961, Acc = 29.91%\n",
      "Iter 1500, Cost= 1.130, Acc = 33.96%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "bought the popper about two years ago and have been enjoying the delicious fresh buttery salty ( as i want ) best popcorn ever . love it and it 's a staple snack in our house . would never <UNK> corn again .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "top notch \n",
      "\n",
      "\n",
      "Iter 1600, Cost= 0.855, Acc = 35.24%\n",
      "Iter 1700, Cost= 0.701, Acc = 38.61%\n",
      "Iter 1800, Cost= 0.865, Acc = 35.64%\n",
      "Iter 1900, Cost= 0.868, Acc = 39.62%\n",
      "Iter 2000, Cost= 0.849, Acc = 40.78%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i love sour stuff . this is n't too sour but still gets the job done . good chewy candy . arrived faster than expected too .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "good \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "mmmmm \n",
      "\n",
      "\n",
      "Iter 2100, Cost= 0.951, Acc = 32.73%\n",
      "Iter 2200, Cost= 0.875, Acc = 31.68%\n",
      "Iter 2300, Cost= 0.866, Acc = 42.20%\n",
      "Iter 2400, Cost= 0.725, Acc = 46.32%\n",
      "Iter 2500, Cost= 0.793, Acc = 35.71%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i had not tried this tea before but i was hoping it was similar to one i tried while in england . i was not disappointed . the pack of 6 makes it a very good value as well .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "tea tea \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "love this tea ! \n",
      "\n",
      "\n",
      "Iter 2600, Cost= 0.864, Acc = 34.82%\n",
      "Iter 2700, Cost= 0.853, Acc = 38.10%\n",
      "Iter 2800, Cost= 0.694, Acc = 40.40%\n",
      "Iter 2900, Cost= 1.020, Acc = 34.26%\n",
      "Iter 3000, Cost= 0.782, Acc = 43.00%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "extremely disappointing . frankly , i think plain old lipton tea is smoother and less bitter . when brewed , i could hardly recognize it as green tea . it tasted more like a very poor earl gray .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "sad \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "not good at all \n",
      "\n",
      "\n",
      "Iter 3100, Cost= 0.756, Acc = 35.64%\n",
      "\n",
      "\n",
      "STARTING VALIDATION\n",
      "\n",
      "\n",
      "Validating data # 0\n",
      "Validating data # 100\n",
      "Validating data # 200\n",
      "Validating data # 300\n",
      "\n",
      "\n",
      "Epoch: 3\n",
      "\n",
      "\n",
      "Average Training Loss: 0.820\n",
      "Average Training Accuracy: 38.18\n",
      "Average Validation Loss: 0.801\n",
      "Average Validation Accuracy: 39.24\n",
      "\n",
      "Model saved\n",
      "\n",
      "\n",
      "\n",
      "STARTING TRAINING\n",
      "\n",
      "\n",
      "Iter 0, Cost= 0.821, Acc = 39.00%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "love this tea . i do not like the plain sleepytime but adding the vanilla is a great move ! highly recommend it . looking forward to trying the honey sleepy time !\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "love ! \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "love it \n",
      "\n",
      "\n",
      "Iter 100, Cost= 0.725, Acc = 37.00%\n",
      "Iter 200, Cost= 0.805, Acc = 39.29%\n",
      "Iter 300, Cost= 0.838, Acc = 41.23%\n",
      "Iter 400, Cost= 0.713, Acc = 49.07%\n",
      "Iter 500, Cost= 0.722, Acc = 37.86%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "the product arrived quickly . all bags and chips were in place ... and safe ; <UNK> these chips are delicious and only four ww points !\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "delicious \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "yum ! \n",
      "\n",
      "\n",
      "Iter 600, Cost= 0.941, Acc = 34.82%\n",
      "Iter 700, Cost= 0.678, Acc = 42.00%\n",
      "Iter 800, Cost= 0.607, Acc = 47.47%\n",
      "Iter 900, Cost= 0.679, Acc = 41.94%\n",
      "Iter 1000, Cost= 0.763, Acc = 48.60%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "this is a light to medium roast , wish it was slightly stronger , but the flavor is good and i am having it every morning using 2 6 oz . <UNK> pumps to make it as strong as possible .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "very good \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "i like it ! \n",
      "\n",
      "\n",
      "Iter 1100, Cost= 0.671, Acc = 44.44%\n",
      "Iter 1200, Cost= 0.810, Acc = 39.81%\n",
      "Iter 1300, Cost= 0.899, Acc = 31.78%\n",
      "Iter 1400, Cost= 0.865, Acc = 39.42%\n",
      "Iter 1500, Cost= 0.809, Acc = 36.54%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i expected a little more flavor as i usually like green mountain <UNK> < br / > next time i 'll look for a french roast !\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "good \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "too weak \n",
      "\n",
      "\n",
      "Iter 1600, Cost= 0.873, Acc = 39.45%\n",
      "Iter 1700, Cost= 0.882, Acc = 38.14%\n",
      "Iter 1800, Cost= 0.953, Acc = 34.86%\n",
      "Iter 1900, Cost= 0.961, Acc = 33.66%\n",
      "Iter 2000, Cost= 0.774, Acc = 35.92%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i use this sauce on pork ribs , after baking them at 300 degrees for 3 hours . the sweet taste of honey along with the tomato is heavenly .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great sauce \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "the best \n",
      "\n",
      "\n",
      "Iter 2100, Cost= 0.744, Acc = 39.13%\n",
      "Iter 2200, Cost= 0.697, Acc = 41.58%\n",
      "Iter 2300, Cost= 0.869, Acc = 34.26%\n",
      "Iter 2400, Cost= 0.867, Acc = 31.48%\n",
      "Iter 2500, Cost= 0.784, Acc = 38.14%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "excellent < a <UNK> '' http : <UNK> '' > kellogg 's cereal in a cup , favorite assortment pack , 1.5 - <UNK> <UNK> cups ( pack of 60 ) < <UNK> >\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "good \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "kelloggs \n",
      "\n",
      "\n",
      "Iter 2600, Cost= 0.653, Acc = 45.45%\n",
      "Iter 2700, Cost= 0.713, Acc = 46.73%\n",
      "Iter 2800, Cost= 0.777, Acc = 39.05%\n",
      "Iter 2900, Cost= 0.795, Acc = 38.10%\n",
      "Iter 3000, Cost= 0.802, Acc = 41.12%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "this is a good product . the honey tastes great , and it 's very convenient and <UNK> . my local <UNK> store was trying to sell this to me for twice the price as amazon , so i 'm pretty sure this is a good buy .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "honey ! \n",
      "\n",
      "\n",
      "Iter 3100, Cost= 0.773, Acc = 45.54%\n",
      "\n",
      "\n",
      "STARTING VALIDATION\n",
      "\n",
      "\n",
      "Validating data # 0\n",
      "Validating data # 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating data # 200\n",
      "Validating data # 300\n",
      "\n",
      "\n",
      "Epoch: 4\n",
      "\n",
      "\n",
      "Average Training Loss: 0.804\n",
      "Average Training Accuracy: 39.03\n",
      "Average Validation Loss: 0.786\n",
      "Average Validation Accuracy: 40.62\n",
      "\n",
      "Model saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "with tf.Session() as sess:  # Start Tensorflow Session\n",
    "    display_step = 100\n",
    "    patience = 5\n",
    "\n",
    "    load = input(\"\\nLoad checkpoint? y/n: \")\n",
    "    print(\"\")\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    if load.lower() == 'y':\n",
    "\n",
    "        print('Loading pre-trained weights for the model...')\n",
    "\n",
    "        train_saver.restore(sess, 'Model_Backup/Seq2seq_summarization.ckpt')\n",
    "        sess.run(tf.global_variables())\n",
    "        sess.run(tf.tables_initializer())\n",
    "\n",
    "        with open('Model_Backup/Seq2seq_summarization.pkl', 'rb') as fp:\n",
    "            train_data = pickle.load(fp)\n",
    "\n",
    "        covered_epochs = train_data['covered_epochs']\n",
    "        best_loss = train_data['best_loss']\n",
    "        impatience = 0\n",
    "        \n",
    "        print('\\nRESTORATION COMPLETE\\n')\n",
    "\n",
    "    else:\n",
    "        best_loss = 2**30\n",
    "        impatience = 0\n",
    "        covered_epochs = 0\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        sess.run(tf.tables_initializer())\n",
    "\n",
    "    epoch=0\n",
    "    while (epoch+covered_epochs)<epochs:\n",
    "        \n",
    "        print(\"\\n\\nSTARTING TRAINING\\n\\n\")\n",
    "        \n",
    "        batches_indices = [i for i in range(0, len(train_batches_text))]\n",
    "        random.shuffle(batches_indices)\n",
    "\n",
    "        total_train_acc = 0\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for i in range(0, len(train_batches_text)):\n",
    "            \n",
    "            j = int(batches_indices[i])\n",
    "\n",
    "            cost,prediction,\\\n",
    "                acc, _ = sess.run([cross_entropy,\n",
    "                                   outputs,\n",
    "                                   accuracy,\n",
    "                                   train_op],\n",
    "                                  feed_dict={tf_text: train_batches_text[j],\n",
    "                                             tf_embd: embd,\n",
    "                                             tf_summary: train_batches_summary[j],\n",
    "                                             tf_true_summary_len: train_batches_true_summary_len[j],\n",
    "                                             tf_train: True})\n",
    "            \n",
    "            total_train_acc += acc\n",
    "            total_train_loss += cost\n",
    "\n",
    "            if i % display_step == 0:\n",
    "                print(\"Iter \"+str(i)+\", Cost= \" +\n",
    "                      \"{:.3f}\".format(cost)+\", Acc = \" +\n",
    "                      \"{:.2f}%\".format(acc*100))\n",
    "            \n",
    "            if i % 500 == 0:\n",
    "                \n",
    "                idx = random.randint(0,len(train_batches_text[j])-1)\n",
    "                \n",
    "                \n",
    "                \n",
    "                text = \" \".join([idx2vocab.get(vec,\"<UNK>\") for vec in train_batches_text[j][idx]])\n",
    "                predicted_summary = [idx2vocab.get(vec,\"<UNK>\") for vec in prediction[idx]]\n",
    "                actual_summary = [idx2vocab.get(vec,\"<UNK>\") for vec in train_batches_summary[j][idx]]\n",
    "                \n",
    "                print(\"\\nSample Text\\n\")\n",
    "                print(text)\n",
    "                print(\"\\nSample Predicted Summary\\n\")\n",
    "                for word in predicted_summary:\n",
    "                    if word == '<EOS>':\n",
    "                        break\n",
    "                    else:\n",
    "                        print(word,end=\" \")\n",
    "                print(\"\\n\\nSample Actual Summary\\n\")\n",
    "                for word in actual_summary:\n",
    "                    if word == '<EOS>':\n",
    "                        break\n",
    "                    else:\n",
    "                        print(word,end=\" \")\n",
    "                print(\"\\n\\n\")\n",
    "                \n",
    "        print(\"\\n\\nSTARTING VALIDATION\\n\\n\")\n",
    "                \n",
    "        total_val_loss=0\n",
    "        total_val_acc=0\n",
    "                \n",
    "        for i in range(0, len(val_batches_text)):\n",
    "            \n",
    "            if i%100==0:\n",
    "                print(\"Validating data # {}\".format(i))\n",
    "\n",
    "            cost, prediction,\\\n",
    "                acc = sess.run([cross_entropy,\n",
    "                                outputs,\n",
    "                                accuracy],\n",
    "                                  feed_dict={tf_text: val_batches_text[i],\n",
    "                                             tf_embd: embd,\n",
    "                                             tf_summary: val_batches_summary[i],\n",
    "                                             tf_true_summary_len: val_batches_true_summary_len[i],\n",
    "                                             tf_train: False})\n",
    "            \n",
    "            total_val_loss += cost\n",
    "            total_val_acc += acc\n",
    "            \n",
    "        avg_val_loss = total_val_loss/len(val_batches_text)\n",
    "        \n",
    "        print(\"\\n\\nEpoch: {}\\n\\n\".format(epoch+covered_epochs))\n",
    "        print(\"Average Training Loss: {:.3f}\".format(total_train_loss/len(train_batches_text)))\n",
    "        print(\"Average Training Accuracy: {:.2f}\".format(100*total_train_acc/len(train_batches_text)))\n",
    "        print(\"Average Validation Loss: {:.3f}\".format(avg_val_loss))\n",
    "        print(\"Average Validation Accuracy: {:.2f}\".format(100*total_val_acc/len(val_batches_text)))\n",
    "              \n",
    "        if (avg_val_loss < best_loss):\n",
    "            best_loss = avg_val_loss\n",
    "            save_data={'best_loss':best_loss,'covered_epochs':covered_epochs+epoch+1}\n",
    "            impatience=0\n",
    "            with open('Model_Backup/Seq2seq_summarization.pkl', 'wb') as fp:\n",
    "                pickle.dump(save_data, fp)\n",
    "            saver.save(sess, 'Model_Backup/Seq2seq_summarization.ckpt')\n",
    "            print(\"\\nModel saved\\n\")\n",
    "              \n",
    "        else:\n",
    "            impatience+=1\n",
    "              \n",
    "        if impatience > patience:\n",
    "              break\n",
    "              \n",
    "              \n",
    "        epoch+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Works\n",
    "\n",
    "* Beam Search\n",
    "* Pointer Mechanisms\n",
    "* BLEU\\ROUGE evaluation\n",
    "* Implement Testing\n",
    "* Complete Training and Optimize Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
